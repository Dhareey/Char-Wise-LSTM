{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> INTRODUCTION</h1>\n",
    "This code runs a Character-Wise-LSTM on Yoruba text. The algorithm will train character by character on the yoruba text and also generate text character by character. I specifically chose Yoruba text because it has special characters in it.\n",
    "<h6>The model</h6>\n",
    "The model is based on Andrej Karpathy's <a href= \"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">post on RNNs</a> and <a href= \"https://github.com/karpathy/char-rnn\">implementation in Torch</a>. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"images/charseq.jpeg\" width=\"500\">\n",
    "<h6>The Data</h6>\n",
    "The training data is an excerpt from yoruba bible. I have written a scrapper to scrap any chapter online in Yoruba. \n",
    "\n",
    "<h6> HOW TO GO ABOUT IT</h6>\n",
    "<ol>\n",
    "    <li>Get the data</li>\n",
    "    <li>The algorithm cant understand strings, so we have to convert it to numbers</li>\n",
    "    <li>Preprocess the data</li>\n",
    "    Since its a character-wise training, each character will be fed into the algorithm, one by one. To pass each character into the algorithm, we have to do one <a href=''>hot encoding</a>. Meaning, we will pass array of 1s and 0s, where the column of the character being passed is 1 and the other column is 0. This will be done for each letter in the data. Meaning, the total number of columns will be the total number of unique character in the text. For example. The letter a will be passed as\n",
    "    <p> [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] and the letter be will be passed as </p>\n",
    "    <p> [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</p>\n",
    "    Where each column represents a unique letter in the alphabet\n",
    "    <li>Since the data is hundreds of thousand character long, we will have to train the algorithm in batches </li>\n",
    "    <li> Define the LSTM architecture</li>\n",
    "    <p> The architecture is pretty basic, one character enters the LSTM at a time. To understand how LSTM works, please chck out the \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapper import getchap\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8x50x97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Getting the data\n",
    "book, validationbook = ['gal','heb','rom','rev','mat','mrk','luk','jhn'], ['heb', 'rom']\n",
    "chapter, validchapter = 6, 9\n",
    "data, validData= [], []\n",
    "for eachbook in book:\n",
    "    for i in range(chapter):\n",
    "        data.append(getchap(eachbook, str(i+1)))\n",
    "        \n",
    "for eachbook in validationbook:\n",
    "    for i in range(6,validchapter):\n",
    "        validData.append(getchap(eachbook, str(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas, valdatas = '\\n'.join(data), '\\n'.join(validData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Paulu Aposteli \n",
      "1 Paulu, aposteli tí a rán kì í ṣe láti ọ̀dọ̀ ènìyàn wá, tàbí nípa ènìyàn, ṣùgbọ́n nípa Jesu Kristi àti Ọlọ́run Baba, ẹni tí ó jí i dìde kúrò nínú òkú. \n",
      "2 Àti gbogbo àwọn arákùnrin t\n"
     ]
    }
   ],
   "source": [
    "#Visualise the data\n",
    "print(datas[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Tokenise the data\n",
    "uniquedata = tuple(set(datas))\n",
    "datacode = {alphabet:code for code,alphabet in dict(enumerate(uniquedata)).items()}\n",
    "encodedData = np.array([datacode[word] for word in datas])\n",
    "encodedvalData = np.array([datacode[word] for word in valdatas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68,  1, 35, 17, 42, 26, 42,  1, 67,  3,  6, 41, 61, 71, 26, 79,  1,\n",
       "       45, 68,  1, 35, 17, 42, 26, 42, 10,  1, 17,  3,  6, 41, 61, 71, 26,\n",
       "       79,  1, 61, 43,  1, 17,  1, 90, 49, 92,  1, 33,  5,  1, 43,  1,  4,\n",
       "       71,  1, 26, 49, 61, 79,  1, 81, 65, 94, 81, 65,  1, 40, 92,  5, 16,\n",
       "        2, 92,  1, 30, 49, 10,  1, 61,  2, 15, 43,  1, 92, 43,  3, 17,  1,\n",
       "       40, 92,  5, 16,  2, 92, 10,  1,  4, 93,  9, 15, 81, 39, 92,  1, 92,\n",
       "       43,  3, 17,  1, 31, 71, 41, 42,  1, 19, 90, 79, 41, 61, 79,  1,  2,\n",
       "       61, 79,  1, 48, 26, 81, 39, 90, 42, 92,  1, 84, 17, 15, 17, 10,  1,\n",
       "       95, 92, 79,  1, 61, 43,  1, 62,  1, 75, 43,  1, 79,  1, 94,  5, 94,\n",
       "       71,  1, 33,  0, 90, 11,  1, 92, 43, 92,  0,  1, 11, 33,  0, 69,  1,\n",
       "       45, 59,  1, 77, 61, 79,  1,  9, 15,  6,  9, 15,  6,  1,  2, 30, 81,\n",
       "       92,  1, 17, 90, 49, 33, 93, 92, 90, 79, 92,  1, 61, 43,  1, 62,  1,\n",
       "       30,  2,  1,  3, 95, 65, 26,  0,  1, 53, 79, 10, 80, 43,  1,  2, 30,\n",
       "       81, 92,  1,  5, 75, 81,  1, 92, 43,  1, 50, 17, 26, 17, 61, 79, 17,\n",
       "       63,  1, 45, 86,  1, 58,  6, 90, 71, 28, 81, 65, 87, 95, 39,  1, 41,\n",
       "       43,  1, 16, 43, 92,  1,  2, 61, 79,  1,  2, 26,  2, 49, 87, 43,  2,\n",
       "        1, 26, 49, 61, 79,  1, 81, 65, 94, 81, 65,  1, 48, 26, 81, 39, 90,\n",
       "       42, 92,  1, 84, 17, 15, 17,  1, 30, 49,  1,  2, 61, 79,  1, 31, 71,\n",
       "       41, 42,  1, 19, 90, 79, 41, 61, 79,  1, 58, 26,  0, 30, 17, 10,  1,\n",
       "       45, 24,  1, 95, 92, 79,  1, 61, 43,  1, 62,  1, 87, 79,  1, 11, 42,\n",
       "       92,  1, 61,  5, 33, 49, 90, 17,  1, 90, 95, 65,  1, 94, 43,  3, 11,\n",
       "        1, 95, 65,  4, 95, 65,  1, 30, 17, 10,  1, 33, 43,  1, 62,  1, 26,\n",
       "       40,  1,  9, 15,  2,  1, 30, 49,  1, 33,  0, 90, 11,  1, 92, 43, 92,\n",
       "        0,  1, 17, 16, 13,  1, 15,  0, 15])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualise tokenised data\n",
    "encodedData[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  1, 18, 71, 26, 33, 79, 41, 71, 94, 71, 33, 79,  1, 75, 95, 39,\n",
       "        1,  2, 26, 93, 87, 49,  2,  1, 45, 68,  1, 66, 43, 61,  6, 90, 43,\n",
       "        1, 18, 71, 26, 33, 79, 41, 71, 94, 71, 33, 79,  1, 16,  5, 43, 10,\n",
       "        1, 81, 15, 17,  1, 80, 17, 26, 95, 53, 42, 10,  1,  2, 26, 93, 87,\n",
       "       49,  2,  1, 48, 26, 81, 39, 90, 42, 92,  1, 48, 65,  9, 49, 28, 11,\n",
       "        9,  6, 10,  1, 95, 92, 79,  1, 61, 43,  1, 62,  1,  3,  2, 94, 13,\n",
       "        1, 67, 15, 90, 17, 52, 17, 53, 42,  1, 15, 43,  1, 62,  1, 61, 79,\n",
       "        1, 73,  1,  3, 17, 94,  2,  1, 15, 81, 65,  1, 26, 49, 61, 79,  1,\n",
       "       79, 15, 79,  1,  3, 43,  3, 17,  1,  2, 30, 81, 92,  1, 81, 15, 17,\n",
       "       10,  1, 61, 43,  1, 62,  1, 41,  5,  1, 41,  0, 90, 71,  1, 87,  0,\n",
       "       92,  1, 42, 92, 10,  1, 45, 59,  1, 95, 92, 79,  1, 61, 43,  1, 67,\n",
       "       15, 90, 17, 52, 17, 53, 42,  1, 41,  5,  1,  3, 43, 92,  1,  5, 94,\n",
       "       49, 53, 95, 39, 30,  2, 49,  1,  6, 52, 42, 92,  1,  9, 15,  6,  9,\n",
       "       15,  6,  1, 87,  0, 92, 69,  1, 66, 43,  1, 81, 65, 92,  2,  1, 40,\n",
       "       33, 43, 92, 92, 43,  1,  6, 90,  0, 33, 81,  1, 90, 95, 65,  1, 61,\n",
       "        0, 53, 81, 65,  1, 41, 43,  1, 29, 81, 15, 17,  1, 11, 94,  6, 94,\n",
       "        6, 10, 14,  1,  2, 61, 79,  1, 26, 95, 39, 16,  5, 92,  1, 92, 49,\n",
       "        2,  1,  3, 95, 65, 26,  0, 10,  1, 29, 81, 15, 17,  1, 80, 17, 26,\n",
       "       95, 53, 42, 10, 14,  1, 61, 43,  1, 43,  1,  4, 71,  1, 29, 81, 15,\n",
       "       17,  1,  2, 26,  2, 49, 87, 43,  2, 69, 14,  1, 45, 86,  1, 60, 49,\n",
       "        5, 92, 43,  1, 15, 17, 15, 17, 10,  1, 26, 49,  5, 92, 43,  1,  5,\n",
       "       16, 49, 10,  1, 26, 49,  5, 92, 43,  1,  5, 61,  2, 92,  1,  5, 90,\n",
       "       17, 92, 10,  1, 15, 95, 39, 95, 65,  1, 92, 79,  1, 33, 11,  1, 92,\n",
       "       43,  1,  5, 15, 95, 65, 90, 95, 65])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedvalData[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ydata = list(encodedData[1:])\n",
    "encoded_ydata.append(encodedData[0])\n",
    "validdatalist = list(encodedvalData[1:])\n",
    "validdatalist.append(encodedvalData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedydata= np.array(encoded_ydata)\n",
    "encodedvaliddata = np.array(validdatalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(encodedData), torch.from_numpy(encodedydata))\n",
    "valid_data = TensorDataset(torch.from_numpy(encodedvalData), torch.from_numpy(encodedvaliddata))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 400\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter= iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "validiter = iter(valid_loader)\n",
    "valid_x, valid_y = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Onehot encoding\n",
    "def oneHotEncode(arr, cols_num):\n",
    "    \"\"\"\n",
    "    This function takes in an pytorch dataloader object and returns a one-hot encoded array with dimensions of array x n_labels.\n",
    "    E.G if it takes an array of [3,2,1] and n_labels of 8, it returns a 3x3(array_size by cols_num) hot encoded array like so\n",
    "    [[0 0 1 0 0 0 0 0]\n",
    "    [0 1 0 0 0 0 0 0]\n",
    "    [1 0 0 0 0 0 0 0]]\n",
    "    \"\"\"\n",
    "    array = np.array(arr)\n",
    "    # First, create an array.size by cols_num array of zeros in float\n",
    "    one_hot = np.zeros((array.size,cols_num), dtype= np.float32)\n",
    "    \n",
    "    # Fill a \"1\" to each row based on the value in array\n",
    "    one_hot[np.arange(one_hot.shape[0]), array.flatten()] = 1.\n",
    "    print(one_hot.shape)\n",
    "    \n",
    "    # Return back to the original shape\n",
    "    one_hot = one_hot.reshape((*array.shape, cols_num))\n",
    "    print(one_hot.shape)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 Model Architecture\n",
    "class yorLSTM(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden= 256, n_layers=2,drop_prob=0.5,lr= 0.01):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden= n_hidden\n",
    "        self.lr = lr\n",
    "            \n",
    "        #Here, we are creating character dict for the project\n",
    "        self.chars = tokens\n",
    "        self.int2chars = dict(enumerate(self.chars))\n",
    "        self.char2int = {num:alphabet for alphabet,num in self.int2chars.items()}\n",
    "            \n",
    "        # Define model layers\n",
    "        self.lstm = nn.LSTM(len(self.chars), self.n_hidden, self.n_layers,dropout=drop_prob,batch_frst=True)\n",
    "            \n",
    "        # Dropout in between layers\n",
    "        self.dropout =nn.Dropout(drop_prob)\n",
    "            \n",
    "        # Connect to a fully connected layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Dropout to avoid overfitting\n",
    "        out= self.dropout(out)\n",
    "        \n",
    "        # Reshape for fully connected layer\n",
    "        out = out.view(-1, self.n_hidden)\n",
    "        \n",
    "        # Pass through a fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        #return out, hidden\n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the weight and hidden value\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5 Train the Model\n",
    "def train(network,training_data, validation_data, epochs, batch_size,lr, seq_length=10, clip=5, vis=10):\n",
    "    # Set the RNN network to train\n",
    "    network.train()\n",
    "    \n",
    "    #Set the optimiser and calculate the loss\n",
    "    optimiser = torch.optim.Adam(net.parameters(), lr= lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Run on CUDA if available\n",
    "    if (train_on_gpu):\n",
    "        network.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    # set total number of characters\n",
    "    n_chars = len(network.chars)\n",
    "    \n",
    "    # Train in range of epochs\n",
    "    for i in range(epochs):\n",
    "        # initialise the hidden state\n",
    "        h = network.init_hidden(batch_size)\n",
    "        for x, y in training_data:\n",
    "            counter+=1\n",
    "            \n",
    "            # One-Hot-Encode the training data\n",
    "            x = oneHotEncode(x.reshape(8,50), n_chars)\n",
    "            \n",
    "            # If on Cuda, cnvert the x and y to cuda\n",
    "            if (train_on_gpu):\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # Set accumulated gradient to zero\n",
    "            network.zero_grad()\n",
    "            \n",
    "            output, h = network(x, h)\n",
    "            \n",
    "            # Calculate the loss and back propagate\n",
    "            loss = criterion(output, y.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Calculate validation loss at every 10 iteration\n",
    "            if counter% vis == 0:\n",
    "                # Initialise the hidden state\n",
    "                val_h = network.init_hidden(batch_size)\n",
    "                validation_losses = []\n",
    "                \n",
    "                #set network to evalution\n",
    "                network.eval()\n",
    "                \n",
    "                for x,y in validation_data:\n",
    "                    x = oneHotEncode(x.reshape(8,50), n_chars)\n",
    "                    \n",
    "                    if (train_on_gpu):\n",
    "                        x,y = x.cuda(), y.cuda()\n",
    "                    val_h = tuple([each for each in val_h])\n",
    "                    \n",
    "                    output, val_h = network(x, val_h)\n",
    "                    \n",
    "                    #Calculate the loss\n",
    "                    loss = criterion(output, y.view(batch_size* seq_length).long())\n",
    "                    \n",
    "                    # Append loss to validation losses\n",
    "                    validation_losses.append(loss)\n",
    "                    \n",
    "                    # Set network back to training\n",
    "                    network.train()\n",
    "                    \n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
